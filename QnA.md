Describe in a high level the solution you have in mind
> We need scores for songs in two criteria: tags and follows. Tags: for each song a user listened to, we calculate the number of tag intersections of it with all the other songs in the database, and then sum them up. The best songs are those with the highest number of intersections. Follows: do a breadth-first search in the graph of users as nodes and follows as directed edges, starting with user A. For all users followed by A, score their songs with 1. For all of their followers, score their songs with 1/2, and all of their followers with 1/4, and so on. Limit this search so that it works for huge graphs. Now just use a weighted mean to combine the scores. Return the five (or so) top scoring songs that the user has not listened to.

What other data could you use to improve recommendatons?
> Use song ratings made by my friends, since listening to a song does not mean a user liked it. Maybe all my friends listened to a song and all hated them, so it is not wise to recommend it to me. Also, the users could input how strong a connection they have with other users, not just following. Something like "I really like this user's music taste" or "I like it but not so much".

Assume a more real world situation where you could have more data you described above, and more time to implement, could you think of a possibly more efficient way to recommend?
> Just use these data to put more importance on different followees. When doing the search in the graph, instead of only using distances as weights, combine distances and strength of connections. The additional implementation effort is minimum, assuming the data is already available.

Assume you have more than one implementation of recommendations, how could you test which one is more effective using data generated by user actions?
> Using different recommendation engines for different users (randomly) and comparing the rate of clicks per recommendation for each engine.

--

How long did this assignment take? Please be honest it's relativelly new.
> About 12 hours. This is due to installing node/npm, finding libraries for some tasks (request, async, mocha, supertest), and looking for the current best practices, since I have not used Node in the last months and it is moving fast.

Where would be the bottlenecks of this solution you have implemented?
> The intersections of tags, which uses all songs in the database for every request. If it gets large, this could be a problem. The solution is to index all tags, precalculating the number of songs with each tag. This can be done by a separate process and speeds up queries a lot.

What was the hardest part?
> Getting used to JavaScript again. It has a lot of inconsistencies and strange behaviors that always bite me if I haven't used it for some time :)

Did you learn something new?
> I found the 'request' lib, which turned out to be quite useful.

Do you feel your skills were under tested?
> No, it was a good practical test.
